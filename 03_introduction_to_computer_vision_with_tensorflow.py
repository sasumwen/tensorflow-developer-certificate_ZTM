# -*- coding: utf-8 -*-
"""03-introduction-to-computer-vision-with-tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fSu4j3McBV1cGc_PXGzLEnEDZPb5_zru

# Introduction to convolutional neural networks and computer vision with tensorflow

# Get the data
"""

import zipfile

!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip

# unzip
zip_ref = zipfile.ZipFile("pizza_steak.zip")
zip_ref.extractall()
zip_ref.close()

"""## Inspeact the data"""

!ls pizza_steak

!ls pizza_steak/train

!ls pizza_steak/train/steak

import os

# walk throufgh the pizza_steak dir and list th enumber of files
for dirpath, dirnames, filenames in os.walk("pizza_steak"):
  print(f"there are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.")

"""### Visualize images

"""

# get the classnames programmativcally

import pathlib
import numpy as np
data_dir = pathlib.Path("pizza_steak/train")
class_names = np.array(sorted([item.name for item in data_dir.glob("*")])) # created a list of class_names from the sub directories
print(class_names)

#  lets voisualize
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random

def view_random_image(target_dir, target_class):
  # setup the target dir
  target_folder = target_dir + target_class

  # get random image paht
  random_image = random.sample(os.listdir(target_folder), 1)
  print(random_image)

  # read in the image and plot
  img = mpimg.imread(target_folder + "/" + random_image[0])
  plt.imshow(img)
  plt.title(target_class)
  plt.axis('off');

  print(f"image shape: {img.shape}") # show the shape of the image

  return img

# view random image from the training data set
img = view_random_image(target_dir = "pizza_steak/train/",
                        target_class = "steak")

img

# convert img to tensor
import tensorflow as tf

tf.constant(img)

# get all the pixel values btw 0 and 1 (normalization)
img/255.

"""## end-to-end example"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# set the seed
tf.random.set_seed(42)

# preprocess data (get vals btw 0 &1)
train_datagen = ImageDataGenerator(rescale=1/255.)
validation_datagen = ImageDataGenerator(rescale=1/255.)

# setup paths to our datadirectories
train_dir = "/content/pizza_steak/train"
validation_dir = "/content/pizza_steak/test"

# import data from directories and turn it nto batches
train_data = train_datagen.flow_from_directory(directory=train_dir,
                                                 batch_size=32,
                                                 target_size=(224,224),
                                                 class_mode="binary",
                                                 seed=42)

validation_data = validation_datagen.flow_from_directory(directory = validation_dir,
                                                         batch_size=32,
                                                         target_size=(224,224),
                                                         class_mode="binary",
                                                         seed=42)

# build  a cnn model
model_1 = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(filters=10,
                           kernel_size=3,
                           activation='relu',
                           input_shape=(224,224,3)),
    tf.keras.layers.Conv2D(10,3, activation='relu'),
    tf.keras.layers.MaxPool2D(pool_size=2,
                              padding='valid'),
    tf.keras.layers.Conv2D(10,3, activation='relu'),
    tf.keras.layers.Conv2D(10,3, activation='relu'),
    tf.keras.layers.MaxPool2D(2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# compile the cnn
model_1.compile(loss=tf.keras.losses.binary_crossentropy,
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

# fit the model
history_1 = model_1.fit(train_data,
                        epochs=5,
                        steps_per_epoch=len(train_data),
                        validation_data=validation_data,
                        validation_steps=len(validation_data))

model_1.summary()

"""## 1. Becoming one with the data"""

# visualiza the data
plt.figure()
plt.subplot(1,2,1)
steak_img = view_random_image('pizza_steak/train/', 'steak')
plt.subplot(1,2,2)
pizza_img = view_random_image('pizza_steak/train/', 'pizza')

"""### Preprocess the data"""

# define the director dataset paths
train_dir = 'pizza_steak/train/'
test_dir = 'pizza_steak/test/'

"""next turn our data into batches"""

# create train and test data generatos and scale the data
from tensorflow.keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale = 1./255.)
test_datagen = ImageDataGenerator(rescale = 1./255.)

# load in imagedata from dir and turn in to batches
train_data = train_datagen.flow_from_directory(directory = train_dir,
                                               target_size = (224,224),
                                               class_mode = 'binary',
                                               batch_size = 32)
test_data = test_datagen.flow_from_directory(directory = test_dir,
                                             target_size = (224,224),
                                             class_mode = 'binary',
                                             batch_size=32)

# get a sample  of a training data batch
images, labels = train_data.next()
len(images), len(labels)

# how many batchs
len(train_data), len(test_data)

# get the first two images
images[:2]

images[0].shape

# view the first batch of labels
labels

"""### create a CNN model"""

# imports
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, Flatten, MaxPool2D, Conv2D, Activation
from tensorflow.keras import Sequential

# create the model
model_4 = Sequential([
    Conv2D(filters=10,
           kernel_size=3,
           strides=1,
           padding='valid',
           activation='relu',
           input_shape=(224,224,3)),
    Conv2D(10,3,activation='relu'),
    Conv2D(10,3,activation='relu'),
    Flatten(),
    Dense(1, activation='sigmoid')
])

# compile the model
model_4.compile(loss='binary_crossentropy',
                optimizer = "Adam",
                metrics = ["accuracy"])

# get a sumaary of model
model_4.summary()

"""### fir the model

"""

# check lens
len(train_data), len(test_data)

# fit the model
history_4 = model_4.fit(train_data,
                        epochs=5,
                        steps_per_epoch=len(train_data),
                        validation_data=test_data,
                        validation_steps = len(test_data))

# evaluating model
import pandas as pd
m_4_plot = pd.DataFrame(history_4.history)
m_4_plot.plot(figsize = (10,7))

# plot a validation and training curves separately
def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics
  """
  loss = history.history["loss"]
  val_loss = history.history["val_loss"]

  accuracy = history.history["accuracy"]
  val_accuracy = history.history["val_accuracy"]

  epochs = range(len(history.history["loss"])) # how many epochs did we run for?

  # Plot loss
  plt.plot(epochs, loss, label = "training_loss")
  plt.plot(epochs, val_loss, label= "val_loss")
  plt.title("loss")
  plt.xlabel("epochs")
  plt.legend()

  # plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label = "training_accuracy")
  plt.plot(epochs, val_accuracy, label= "val_accuracy")
  plt.title("accuracy")
  plt.xlabel("epochs")
  plt.legend();

# check out loss and accuracy of model 4
plot_loss_curves(history_4)

